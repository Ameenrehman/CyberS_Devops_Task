name: Deploy to AWS EKS

on:
  push:
    branches:
      - main
    paths:
      - 'terraform/**'
      - 'node-backend-app1/**'
      - 'k8s/**'
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'
      - name: Setup Helm
        uses: azure/setup-helm@v1
        with:
          version: '3.10.0' 
      - name: Add Helm to PATH
        run: echo "/usr/local/bin" >> $GITHUB_PATH
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.5.0

      - name: Terraform Init
        run: |
          cd terraform
          terraform init -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
                        -backend-config="key=tfstate/main1.tfstate" \
                        -backend-config="region=us-east-1"

      - name: Terraform Apply
        run: |
          cd terraform
          terraform apply -auto-approve
      - name: Get Terraform Outputs
        id: tf_outputs
        run: |
          cd terraform
          # Get VPC ID from Terraform outputs and set it as an environment variable
          VPC_ID=$(terraform output -raw vpc_id)
          echo "VPC_ID=$VPC_ID" >> $GITHUB_ENV
          # Cluster name and region are already defined as locals in main.tf
          # and can be directly used in the Helm install command
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2
      
      #- name: Create Namespace
      #  run: |
      #    kubectl create namespace ameen2607-dev --dry-run=client -o yaml | kubectl apply -f -
        # It will create the namespace if it doesn't exist, and do nothing if it does
      - name: Build and Push Docker Image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          ECR_REPOSITORY: myecr-ameen1
          IMAGE_TAG: node-app1-${{ github.sha }}
        run: |
          docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG ./node-backend-app1
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
          echo "IMAGE_TAG=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG" >> $GITHUB_ENV

      - name: Update Kubernetes Manifests
        env:
          IMAGE_TAG: ${{ env.IMAGE_TAG }}
        run: |
          echo "Final image: ${{ env.IMAGE_TAG }}"
          sed "s|593793064016.dkr.ecr.us-east-1.amazonaws.com/myecr-ameen1@sha256:4240419aa95be71ad66633e17296e6002f938de29ee973031c8162c62c85e857|593793064016.dkr.ecr.us-east-1.amazonaws.com/myecr-ameen1:node-app1-${{ github.sha }}|g" k8s/node-backend-deployment.yaml > k8s/node-backend-deployment-patched.yaml
      
      - name: Configure kubeconfig
        run: |
          aws eks update-kubeconfig --region us-east-1 --name ascode-cluster
      - name: Add EKS Helm Repo
        run: helm repo add eks https://aws.github.io/eks-charts

      - name: Update Helm Repos
        run: |
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update eks
      
      - name: Deploy to EKS
        run: |
          kubectl apply -f k8s/node-backend-deployment-patched.yaml
          kubectl apply -f k8s/node-backend-service.yaml
          kubectl apply -f k8s/node-backend-ingress.yaml
        #  kubectl apply -f k8s/prometheus/prometheus-configmap.yaml
        #  kubectl apply -f k8s/prometheus/
        #  kubectl apply -f k8s/grafana/
      
      - name: deploy eks alb via helm
        run: |
          helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
            --set clusterName=ascode-cluster \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller \
            --namespace kube-system \
            --set region=us-east-1 \
            --set vpcId=${{ env.VPC_ID }} \
            --set image.repository=602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon/aws-load-balancer-controller
            
      - name: Destroy K8s Manifests on Failure
        if: failure()
        run: |
          echo "A previous step failed. Attempting to destroy Kubernetes resources..."
          cd k8s 
          kubectl delete -f k8s/node-backend-service.yaml --ignore-not-found=true --wait=false
          kubectl delete -f k8s/node-backend-deployment-patched.yaml --ignore-not-found=true --wait=false
          kubectl delete -f k8s/node-backend-ingress.yaml --ignore-not-found=true --wait=false
        env:
          # Ensure AWS credentials are available for kubectl
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: us-east-1  
      - name: Terraform Destroy on Failure
        # This step will only run if any previous step in this job fails
        if: failure()
        run: |
          echo "A previous step failed. Attempting to destroy Terraform resources..."
          cd terraform
          # Re-initialize Terraform backend, as the previous init might have failed or context changed
          terraform init -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
                         -backend-config="key=tfstate/main1.tfstate" \
                         -backend-config="region=us-east-1"
          # Attempt to destroy with auto-approve.
          # WARNING: Use -auto-approve with extreme caution in production environments.
          # Consider manual intervention or a separate, explicit destroy workflow.
         #terraform destroy -auto-approve
        env:
          # Ensure AWS credentials are available for this step too
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: us-east-1
